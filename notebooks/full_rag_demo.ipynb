{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3fd2d0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8ec51",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from mistralai import Mistral\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (MarkdownHeaderTextSplitter,\n",
    "                                      RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25c637",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeec004",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"Mistral api key not present in .env\")\n",
    "\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "party = \"50PLUS\"\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Hoofdstuk\"),\n",
    "        (\"##\", \"Sectie\"),\n",
    "        (\"###\", \"Subsectie\"),\n",
    "    ],\n",
    "    strip_headers=True,\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \".\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    keep_separator=\"end\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2d122",
   "metadata": {},
   "source": [
    "## Process PDF with Mistral OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836a7cc",
   "metadata": {},
   "source": [
    "### Upload PDF to Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ef58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_filename = f\"Verkiezingsprogramma {party}.pdf\"\n",
    "pdf_filepath = Path.cwd().parent / \"data\" / \"pdfs\" / pdf_filename\n",
    "\n",
    "if not pdf_filepath.exists():\n",
    "    raise ValueError(f\"The file {pdf_filepath} does not exist.\")\n",
    "\n",
    "logger.info(f\"Uploading {pdf_filename} to Mistral...\")\n",
    "uploaded_pdf = mistral_client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": pdf_filename,\n",
    "        \"content\": open(pdf_filepath, \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")\n",
    "document_url = mistral_client.files.get_signed_url(file_id=uploaded_pdf.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f49e8e",
   "metadata": {},
   "source": [
    "### Process uploaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89077c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Running OCR on document {document_url}...\")\n",
    "ocr_result = mistral_client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": document_url.url,\n",
    "    },\n",
    "    include_image_base64=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ddcc9",
   "metadata": {},
   "source": [
    "### Extract markdown from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_markdown = '\\n\\n'.join([page.markdown for page in ocr_result.pages])\n",
    "print(response_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489deb3",
   "metadata": {},
   "source": [
    "## Markdown Cleanup (do manually)\n",
    "The OCR response is pretty accurate, but can still make errors which result in typos or formatting errors.\n",
    "Especially markdown headers are often not structured correctly, so we need to correct it manually.\n",
    "Below, we load the \"cleaned\" and shortened version of the markdown file; this only contains the main chapters, without introductions or summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f136ff5",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265381c",
   "metadata": {},
   "source": [
    "### Split markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_markdown_file = Path.cwd().parent / \"data\" / \"markdown_short\" / f\"{party}_short.md\"\n",
    "\n",
    "with open(short_markdown_file, 'r', encoding='utf-8') as file:\n",
    "    markdown_string = file.read()\n",
    "\n",
    "# Step 1: Split the markdown text by headers\n",
    "md_header_splits = markdown_splitter.split_text(markdown_string)\n",
    "\n",
    "# Step 2: Recursively split the header chunks into smaller chunks\n",
    "chunks = recursive_splitter.split_documents(md_header_splits)\n",
    "\n",
    "example_chunk = chunks[20]\n",
    "example_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4280cba",
   "metadata": {},
   "source": [
    "### Chunks visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c78361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_dataframe(chunks: list[Document]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a list of LangChain Document objects into a pandas DataFrame,\n",
    "    extracting specified metadata fields.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        metadata = chunk.metadata\n",
    "        page_content = chunk.page_content\n",
    "\n",
    "        # Extract metadata fields with a default value of None or an empty string\n",
    "        hoofdstuk = metadata.get('Hoofdstuk', \"\")\n",
    "        sectie = metadata.get('Sectie', \"\")\n",
    "        subsectie = metadata.get('Subsectie', \"\")\n",
    "\n",
    "        data.append({\n",
    "            'Partij': party,\n",
    "            'Hoofdstuk': hoofdstuk,\n",
    "            'Sectie': sectie,\n",
    "            'Subsectie': subsectie,\n",
    "            'Text': page_content,\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d08a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = chunks_to_dataframe(chunks)[:50]\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91526146",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6316fe",
   "metadata": {},
   "source": [
    "### Process document chunks for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108db435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_embedding_content(chunk: Document) -> str:\n",
    "    metadata = chunk.metadata\n",
    "    page_content = chunk.page_content\n",
    "\n",
    "    chapter_title = metadata.get(\"Hoofdstuk\")\n",
    "    if not chapter_title:\n",
    "        raise ValueError(\"No chapter title found\")\n",
    "\n",
    "    section_title = metadata.get(\"Sectie\", \"\")\n",
    "    subsection_title = metadata.get(\"Subsectie\", \"\")\n",
    "\n",
    "    embedding_content = f\"{chapter_title}\\n{section_title}\\n{subsection_title}\\n{page_content}\"\n",
    "\n",
    "    return embedding_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_content = format_embedding_content(example_chunk)\n",
    "\n",
    "print(embedding_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ac26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db[\"embedding_content\"] = vector_db[\"Hoofdstuk\"] + '\\n' + vector_db[\"Sectie\"] + '\\n' + vector_db[\"Subsectie\"] + '\\n' + vector_db[\"Text\"]\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19931d",
   "metadata": {},
   "source": [
    "### Embed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "embedding_result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=list(vector_db[\"embedding_content\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb548ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = [item.values for item in embedding_result.embeddings]\n",
    "vector_db['embeddings'] = embedding_list\n",
    "display(vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bc4d1",
   "metadata": {},
   "source": [
    "### Process db query\n",
    "TODO: improve logic and add threshold filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_best_passage(query, dataframe, num_results: int = 3, threshold: float = 0):\n",
    "  \"\"\"\n",
    "  Compute the distances between the query and each document in the dataframe\n",
    "  using the dot product.\n",
    "  \"\"\"\n",
    "  query_embedding = client.models.embed_content(\n",
    "      model=\"gemini-embedding-001\",\n",
    "      contents=query,\n",
    "      config=types.EmbedContentConfig(\n",
    "          task_type=\"retrieval_query\",\n",
    "          )\n",
    "  )\n",
    "\n",
    "  dot_products = np.dot(\n",
    "      np.stack(dataframe['embeddings']),\n",
    "      query_embedding.embeddings[0].values\n",
    "  )\n",
    "  dataframe[\"dot_product\"] = dot_products\n",
    "  index_ranking = np.argsort(dot_products)\n",
    "  top_indices = index_ranking[-num_results:][::-1]\n",
    "  return dataframe.iloc[top_indices] # Return text from index with max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa856ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wat vind de partij van kunstmatige intelligentie?\"\n",
    "\n",
    "top_df = find_best_passage(query, vector_db, num_results=3, threshold=0.6)\n",
    "display(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in top_df.iterrows():\n",
    "    display(result[\"embedding_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bdae5",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "TODO: clean up, improve prompt, define format_return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def make_prompt(query, relevant_passages):\n",
    "  escaped = (\n",
    "      '\\n\\n'.join(relevant_passages)\n",
    "      .replace(\"'\", \"\")\n",
    "      .replace('\"', \"\")\n",
    "      # .replace(\"\\n\", \" \")\n",
    "  )\n",
    "  prompt = textwrap.dedent(\"\"\"\n",
    "    You are a helpful and informative bot that answers questions using text\n",
    "    from the reference passage included below. Be sure to respond in a\n",
    "    complete sentence, being comprehensive, including all relevant\n",
    "    background information.\n",
    "\n",
    "    However, you are talking to a non-technical audience, so be sure to\n",
    "    break down complicated concepts and strike a friendly and conversational\n",
    "    tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "    QUESTION: '{query}'\n",
    "                           \n",
    "    PASSAGES:\n",
    "    {relevant_passage}\n",
    "\n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result_text(result_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Combines Header titles and text into a readable format for each entry\"\"\"\n",
    "    source_list = []\n",
    "    for i, r in result_df.iterrows():\n",
    "        source = f\"\"\"\n",
    "        Bron: {r[\"Partij\"]}-{i}\n",
    "        Hoofdstuk: {r[\"Hoofdstuk\"]}\n",
    "        Sectie: {r[\"Sectie\"] or \"N/A\"}\n",
    "        Subsectie: {r[\"Subsectie\"] or \"N/A\"}\n",
    "        \n",
    "        Text: {r[\"Text\"]}\n",
    "        \"\"\"\n",
    "        source_list.append(source)\n",
    "\n",
    "    return source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_passages = format_result_text(top_df)\n",
    "\n",
    "prompt = make_prompt(query, retrieved_passages)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    ")\n",
    "print(answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694b48f",
   "metadata": {},
   "source": [
    "## pgvector\n",
    "NOTE: Make sure you run the docker-compose command and have env vars set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f2835",
   "metadata": {},
   "source": [
    "### database setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8afe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pg_host = os.getenv(\"PG_HOST\")\n",
    "pg_port = os.getenv(\"PG_PORT\")\n",
    "pg_database = os.getenv(\"PG_DATABASE\")\n",
    "pg_username = os.getenv(\"PG_USERNAME\")\n",
    "pg_password = os.getenv(\"PG_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193014d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=pg_database,\n",
    "        user=pg_username,\n",
    "        password=pg_password,\n",
    "        host=pg_host,\n",
    "        port=pg_port\n",
    "    )\n",
    "    # conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "        # Enable the pgvector extension\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    print(\"pgvector extension enabled.\")\n",
    "\n",
    "    # Create the table\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS political_documents (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        chunk_text TEXT,\n",
    "        party_name TEXT,\n",
    "        document_chapter TEXT,\n",
    "        document_section TEXT,\n",
    "        document_subsection TEXT,\n",
    "        embedding VECTOR(3072)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    print(\"Table 'political_documents' created successfully.\")\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"An error occurred during initial setup:\", e)\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac431f",
   "metadata": {},
   "source": [
    "### Database population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6670d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_insert = \"\"\"\n",
    "INSERT INTO political_documents (chunk_text, party_name, document_chapter, document_section, document_subsection, embedding)\n",
    "VALUES (%s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "pg_vector_data_list = [\n",
    "    (r[\"Text\"], r[\"Partij\"], r[\"Hoofdstuk\"], r[\"Sectie\"], r[\"Subsectie\"], r[\"embeddings\"]) for _, r in vector_db.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db104a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Your database connection and cursor setup\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=pg_database,\n",
    "        user=pg_username,\n",
    "        password=pg_password,\n",
    "        host=pg_host,\n",
    "        port=pg_port\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Roll back any previous failed transaction\n",
    "    conn.rollback()\n",
    "\n",
    "    # Now, run your executemany statement\n",
    "    cursor.executemany(sql_insert, pg_vector_data_list)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    # If the new command fails, make sure to roll back again\n",
    "    conn.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the connection is closed\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4f9b9",
   "metadata": {},
   "source": [
    "### Database retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, top_k=5):\n",
    "    # 1. Generate embedding for the query\n",
    "    query_embedding_response = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=query,\n",
    "        config=types.EmbedContentConfig(\n",
    "            task_type=\"retrieval_query\",\n",
    "            )\n",
    "    )\n",
    "    query_vector = query_embedding_response.embeddings[0].values\n",
    "\n",
    "    # 2. SQL query for nearest neighbors\n",
    "    # Use L2 distance for this example. The '<->' operator is a simple way\n",
    "    # to find nearest neighbors.\n",
    "    sql_query = \"\"\"\n",
    "    SELECT chunk_text, party_name, document_chapter, document_section, document_subsection\n",
    "    FROM political_documents\n",
    "    ORDER BY embedding <-> %s::vector\n",
    "    LIMIT %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your database connection and cursor setup\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=pg_database,\n",
    "        user=pg_username,\n",
    "        password=pg_password,\n",
    "        host=pg_host,\n",
    "        port=pg_port\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql_query, (query_vector, top_k))\n",
    "    results = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ec62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "retrieved_chunks = retrieve_chunks(\"Wat vind de partij van kunstmatige intelligentie?\")\n",
    "rag_context_format = \"Party: {party}\\nChapter: {chapter}\\nSection: {section}\\nSubsection: {subsection}\\n\\nText: {text}\\n---\"\n",
    "rag_context = [\n",
    "    rag_context_format.format(text=text, party=party, chapter=chapter, section=section, subsection=subsection)\n",
    "    for text, party, chapter, section, subsection in retrieved_chunks]\n",
    "print(rag_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_passages = format_result_text(top_df)\n",
    "\n",
    "prompt = make_prompt(query, rag_context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    ")\n",
    "print(answer.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "partij_programma_wijzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
