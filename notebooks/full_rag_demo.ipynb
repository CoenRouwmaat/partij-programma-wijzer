{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3fd2d0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8ec51",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from mistralai import Mistral\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (MarkdownHeaderTextSplitter,\n",
    "                                      RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25c637",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeec004",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"Mistral api key not present in .env\")\n",
    "\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "party = \"50PLUS\"\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Hoofdstuk\"),\n",
    "        (\"##\", \"Sectie\"),\n",
    "        (\"###\", \"Subsectie\"),\n",
    "    ],\n",
    "    strip_headers=True,\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \".\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    keep_separator=\"end\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2d122",
   "metadata": {},
   "source": [
    "## Process PDF with Mistral OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836a7cc",
   "metadata": {},
   "source": [
    "### Upload PDF to Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ef58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_filename = f\"Verkiezingsprogramma {party}.pdf\"\n",
    "pdf_filepath = Path.cwd().parent / \"data\" / \"pdfs\" / pdf_filename\n",
    "\n",
    "if not pdf_filepath.exists():\n",
    "    raise ValueError(f\"The file {pdf_filepath} does not exist.\")\n",
    "\n",
    "logger.info(f\"Uploading {pdf_filename} to Mistral...\")\n",
    "uploaded_pdf = mistral_client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": pdf_filename,\n",
    "        \"content\": open(pdf_filepath, \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")\n",
    "document_url = mistral_client.files.get_signed_url(file_id=uploaded_pdf.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f49e8e",
   "metadata": {},
   "source": [
    "### Process uploaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89077c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Running OCR on document {document_url}...\")\n",
    "ocr_result = mistral_client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": document_url.url,\n",
    "    },\n",
    "    include_image_base64=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ddcc9",
   "metadata": {},
   "source": [
    "### Extract markdown from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_markdown = '\\n\\n'.join([page.markdown for page in ocr_result.pages])\n",
    "print(response_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489deb3",
   "metadata": {},
   "source": [
    "## Markdown Cleanup (do manually)\n",
    "The OCR response is pretty accurate, but can still make errors which result in typos or formatting errors.\n",
    "Especially markdown headers are often not structured correctly, so we need to correct it manually.\n",
    "Below, we load the \"cleaned\" and shortened version of the markdown file; this only contains the main chapters, without introductions or summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f136ff5",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265381c",
   "metadata": {},
   "source": [
    "### Split markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_markdown_file = Path.cwd().parent / \"data\" / \"markdown_short\" / f\"{party}_short.md\"\n",
    "\n",
    "with open(short_markdown_file, 'r', encoding='utf-8') as file:\n",
    "    markdown_string = file.read()\n",
    "\n",
    "# Step 1: Split the markdown text by headers\n",
    "md_header_splits = markdown_splitter.split_text(markdown_string)\n",
    "\n",
    "# Step 2: Recursively split the header chunks into smaller chunks\n",
    "chunks = recursive_splitter.split_documents(md_header_splits)\n",
    "\n",
    "example_chunk = chunks[20]\n",
    "example_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4280cba",
   "metadata": {},
   "source": [
    "### Chunks visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c78361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_dataframe(chunks: list[Document]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a list of LangChain Document objects into a pandas DataFrame,\n",
    "    extracting specified metadata fields.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        metadata = chunk.metadata\n",
    "        page_content = chunk.page_content\n",
    "\n",
    "        # Extract metadata fields with a default value of None or an empty string\n",
    "        hoofdstuk = metadata.get('Hoofdstuk', \"\")\n",
    "        sectie = metadata.get('Sectie', \"\")\n",
    "        subsectie = metadata.get('Subsectie', \"\")\n",
    "\n",
    "        data.append({\n",
    "            'Partij': party,\n",
    "            'Hoofdstuk': hoofdstuk,\n",
    "            'Sectie': sectie,\n",
    "            'Subsectie': subsectie,\n",
    "            'Text': page_content,\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d08a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = chunks_to_dataframe(chunks)[:50]\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91526146",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6316fe",
   "metadata": {},
   "source": [
    "### Process document chunks for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108db435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_embedding_content(chunk: Document) -> str:\n",
    "    metadata = chunk.metadata\n",
    "    page_content = chunk.page_content\n",
    "\n",
    "    chapter_title = metadata.get(\"Hoofdstuk\")\n",
    "    if not chapter_title:\n",
    "        raise ValueError(\"No chapter title found\")\n",
    "\n",
    "    section_title = metadata.get(\"Sectie\", \"\")\n",
    "    subsection_title = metadata.get(\"Subsectie\", \"\")\n",
    "\n",
    "    embedding_content = f\"{chapter_title}\\n{section_title}\\n{subsection_title}\\n{page_content}\"\n",
    "\n",
    "    return embedding_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_content = format_embedding_content(example_chunk)\n",
    "\n",
    "print(embedding_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ac26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db[\"embedding_content\"] = vector_db[\"Hoofdstuk\"] + '\\n' + vector_db[\"Sectie\"] + '\\n' + vector_db[\"Subsectie\"] + '\\n' + vector_db[\"Text\"]\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19931d",
   "metadata": {},
   "source": [
    "### Embed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "embedding_result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=list(vector_db[\"embedding_content\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb548ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = [item.values for item in embedding_result.embeddings]\n",
    "vector_db['embeddings'] = embedding_list\n",
    "display(vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bc4d1",
   "metadata": {},
   "source": [
    "### Process db query\n",
    "TODO: improve logic and add threshold filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_best_passage(query, dataframe, num_results: int = 3, threshold: float = 0):\n",
    "  \"\"\"\n",
    "  Compute the distances between the query and each document in the dataframe\n",
    "  using the dot product.\n",
    "  \"\"\"\n",
    "  query_embedding = client.models.embed_content(\n",
    "      model=\"gemini-embedding-001\",\n",
    "      contents=query,\n",
    "      config=types.EmbedContentConfig(\n",
    "          task_type=\"retrieval_query\",\n",
    "          )\n",
    "  )\n",
    "\n",
    "  dot_products = np.dot(\n",
    "      np.stack(dataframe['embeddings']),\n",
    "      query_embedding.embeddings[0].values\n",
    "  )\n",
    "  dataframe[\"dot_product\"] = dot_products\n",
    "  index_ranking = np.argsort(dot_products)\n",
    "  top_indices = index_ranking[-num_results:][::-1]\n",
    "  return dataframe.iloc[top_indices] # Return text from index with max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa856ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Wat vind de partij van kunstmatige intelligentie?\"\n",
    "\n",
    "top_df = find_best_passage(query, vector_db, num_results=3, threshold=0.6)\n",
    "display(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in top_df.iterrows():\n",
    "    display(result[\"embedding_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bdae5",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "TODO: clean up, improve prompt, define format_return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def make_prompt(query, relevant_passages):\n",
    "  escaped = (\n",
    "      relevant_passages\n",
    "      .replace(\"'\", \"\")\n",
    "      .replace('\"', \"\")\n",
    "      .replace(\"\\n\", \" \")\n",
    "  )\n",
    "  prompt = textwrap.dedent(\"\"\"\n",
    "    You are a helpful and informative bot that answers questions using text\n",
    "    from the reference passage included below. Be sure to respond in a\n",
    "    complete sentence, being comprehensive, including all relevant\n",
    "    background information.\n",
    "\n",
    "    However, you are talking to a non-technical audience, so be sure to\n",
    "    break down complicated concepts and strike a friendly and conversational\n",
    "    tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "    QUESTION: '{query}'\n",
    "                           \n",
    "    PASSAGES: '{relevant_passage}'\n",
    "\n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "\n",
    "\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_return_text(result_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Combines Header titles and text into a readable format for each entry\"\"\"\n",
    "    # TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_passages = format_return_text(top_df)\n",
    "\n",
    "prompt = make_prompt(query, retrieved_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    ")\n",
    "print(answer.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "partij_programma_wijzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
