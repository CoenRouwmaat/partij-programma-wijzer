{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2ae77f",
   "metadata": {},
   "source": [
    "This notebook explores options for finding the page number for any passage / chunk. The process is as follows:\n",
    "- given a chunk, get the headers from the metadata\n",
    "- In the mistral ocr result, find a match for the largest header\n",
    "- If found, find the corresponding page number\n",
    "- Look for the second largest header; if found replace the found page number with this page number\n",
    "- repeat for the third header.\n",
    "\n",
    "Using this technique, this code should find the page number for the smalles header containing this text.\n",
    "Note that it's also possible to look for matches for the chunk text itself, but this becomes tricky when a chunk is split over multiple pages.\n",
    "\n",
    "Since the markdown file is manually edited, it might be possible that header names are edited as well, so the code uses fuzzy matching.\n",
    "\n",
    "TODO: currently very inefficient. Rewrite to process all chunks at once; in order:\n",
    "- start at page 1, find first match\n",
    "- starting at that page, find next match etc\n",
    "- This prevents looking through the entire doc for every chunk\n",
    "\n",
    "TODO: define page_offset for every party to align with page number in document\n",
    "\n",
    "NOTE: If ocr mistakes are made for the header, fuzzy matching needs to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from src.config import FilePaths\n",
    "from src.document_chunking import chunk_markdown_file\n",
    "from src.enums import Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc736a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "party = Party.CDA\n",
    "\n",
    "clean_markdown_file = FilePaths.clean_markdown_dir / f\"{party}_clean.md\"\n",
    "\n",
    "with open(clean_markdown_file, 'r', encoding='utf-8') as file:\n",
    "    markdown_string = file.read()\n",
    "\n",
    "chunks = chunk_markdown_file(markdown_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5459bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = chunks[100]\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def find_page_index_for_chunk(\n",
    "    chunk: Document,\n",
    "    pages,\n",
    "    confidence_threshold: int = 60\n",
    ") -> int:\n",
    "    \n",
    "    header_hierarchy = [\"Hoofdstuk\", \"Sectie\", \"Subsectie\"]\n",
    "\n",
    "    found_index = -1\n",
    "    best_match_score = 0\n",
    "    chunk_metadata = chunk.metadata\n",
    "\n",
    "    for header_type in header_hierarchy:\n",
    "        header_title = chunk_metadata.get(header_type)\n",
    "        if not header_title:\n",
    "            continue\n",
    "\n",
    "        print(\"Looking for\", header_title)\n",
    "\n",
    "        regex_pattern = r\"^#{1,3}\\s.*\"\n",
    "        \n",
    "        # Iterate over pages to find a match\n",
    "        for page in pages:\n",
    "            markdown_text = page.get(\"markdown\", \"\")\n",
    "\n",
    "            # Find all headers in a page\n",
    "            page_headers = re.findall(regex_pattern, markdown_text, re.MULTILINE)\n",
    "\n",
    "            for header in page_headers:\n",
    "                header_text = re.sub(r\"^#{1,3}\\s\", \"\", header).strip()\n",
    "                match_score = fuzz.partial_ratio(header_title, header_text)\n",
    "            \n",
    "                if match_score >= confidence_threshold and match_score >= best_match_score:\n",
    "                    best_match_score = match_score\n",
    "                    found_index = page.get(\"index\", -1)\n",
    "                    print(header, match_score, found_index)\n",
    "                    break\n",
    "    return found_index\n",
    "    \n",
    "\n",
    "def add_chunk_pages_to_metadata(\n",
    "    raw_ocr_path: Path,\n",
    "    chunks: list[Document]\n",
    "):\n",
    "    try:\n",
    "        with open(raw_ocr_path, 'r', encoding='utf-8') as f:\n",
    "            ocr_data = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return -1\n",
    "    \n",
    "    pages = ocr_data.get(\"pages\", [])\n",
    "    if not pages:\n",
    "        return -1\n",
    "\n",
    "    current_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        remaining_pages = pages[current_index:current_index+3]\n",
    "\n",
    "        page_index = find_page_index_for_chunk(\n",
    "            chunk=chunk,\n",
    "            pages=remaining_pages\n",
    "        )\n",
    "        print(\"Found page index: \", page_index)\n",
    "        chunk.metadata[\"Pagina\"] = page_index\n",
    "        if page_index != -1:\n",
    "            current_index = page_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc12179",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_chunk_pages_to_metadata(\n",
    "    raw_ocr_path=FilePaths.json_dir / f\"{party}.json\",\n",
    "    chunks=chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb027af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if chunk.metadata[\"Pagina\"] == -1:\n",
    "        print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "partij_programma_wijzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
